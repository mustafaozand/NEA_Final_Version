{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.320058Z",
     "start_time": "2026-01-28T23:11:37.288760Z"
    }
   },
   "source": [
    "# Below are the libraries that will be used to perform tensor operations for normalisation, matrix-multiplication\n",
    "\n",
    "# The pytorch library will be used to load the datasets, e.g. MNIST and CIFAR-10\n",
    "\n",
    "import torch\n",
    "from sympy.codegen.ast import int8\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 300
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.339036Z",
     "start_time": "2026-01-28T23:11:37.326311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class Model:\n",
    "#     def __init__(self, sequential, optimiser, lr=0.01 ,device=DEVICE):\n",
    "#         self.model_structure = sequential\n",
    "#         self.optimiser = optimiser\n",
    "#         self.device = device\n",
    "#         self.lr = lr\n",
    "#         self.cross_entropy = CrossEntropyLoss()\n",
    "#         self.y_true = None\n",
    "#\n",
    "#\n",
    "#     def train(self, y_true, input):\n",
    "#         self.y_true = y_true\n",
    "#         pred = self.model_structure.forward(input)\n",
    "#         curr_loss = self.cross_entropy.forward(pred, self.y_true)\n",
    "#         print(f\"Current loss: {curr_loss}\")\n",
    "#         grad = self.cross_entropy_loss.backward()\n",
    "#\n",
    "#         for layer in range (len(self.model_structure.layers)-1,0,-1):\n",
    "#             grad = layer.backward(grad)\n",
    "#\n",
    "#             if not isinstance(layer, Softmax):\n",
    "#                 layer.weights = self.optimiser.step(grads=grad, input=layer.weights)\n",
    "#                 layer.bias = self.optimiser.step(grads=grad, input=layer.bias)\n",
    "#\n",
    "#         return curr_loss\n"
   ],
   "id": "3173ce32cde94713",
   "outputs": [],
   "execution_count": 301
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.355560Z",
     "start_time": "2026-01-28T23:11:37.339736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Like the previous numpy version,\n",
    "# for the shape of my input each row = new batch, and each column = neuron (feature)\n",
    "# Sequential class manages the structure of the neural network.\n",
    "# Instead of hard-coding layers, they are defined like this:\n",
    "# model_features = [\n",
    "#     (\"linear\", 784, 128),\n",
    "#     (\"activation\",\"ReLU\"),\n",
    "#     (\"linear\", 128, 10),\n",
    "# ]\n",
    "class Sequential:\n",
    "    def __init__(self, model_features: list, device=DEVICE):\n",
    "        # store the model configuration list for the neural network architecture\n",
    "        self.model_features = model_features\n",
    "        # store the device so that all tensors can be created on the same device\n",
    "        # devices are (mps, cpu, cuda)\n",
    "        self.device = device\n",
    "        # holds the actual layer objects\n",
    "        self.layers = []\n",
    "        # this dictionary is used to map a.f. to their class implementations\n",
    "        self.activations = {\n",
    "            \"ReLU\": ReLU,\n",
    "            \"Sigmoid\": Sigmoid,\n",
    "            \"Tanh\": Tanh,\n",
    "            \"Softmax\": Softmax,\n",
    "        }\n",
    "    # Loop through the model_features to instantiate layers\n",
    "    def create_instances(self):\n",
    "        for item in self.model_features:\n",
    "            layer_type = item[0]\n",
    "            # create a linear layer\n",
    "            if layer_type == \"linear\":\n",
    "                _, in_features, out_features = item\n",
    "                new_layer = Linear(out_features, in_features, device=self.device)\n",
    "                self.layers.append(new_layer)\n",
    "            # set the activation function, technically it's not a layer, but it will still be stored inside layers\n",
    "            elif layer_type == \"activation\":\n",
    "                activation_name = item[1]\n",
    "                self.layers.append(self.activations[activation_name]())\n",
    "                print(f\"Created Activation function: {activation_name}\")\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"to\"):\n",
    "                layer.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # def backward(self, y):\n",
    "    #     for layer in self.layers:\n",
    "    #         y = layer.backward(y)\n",
    "    #\n",
    "    #     return y\n"
   ],
   "id": "1c56f0812e61c739",
   "outputs": [],
   "execution_count": 302
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.370231Z",
     "start_time": "2026-01-28T23:11:37.363518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear:\n",
    "    def __init__(self, outputs, inputs, device=DEVICE):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.device = torch.device(device)\n",
    "        self.weights = torch.randn(outputs, inputs, device=self.device) * 0.01\n",
    "        self.bias = torch.zeros(outputs, device=device)\n",
    "        self.grad_input = None\n",
    "        self.grad_weights = None\n",
    "        self.grad_biases = None\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = torch.device(device)\n",
    "        self.weights = self.weights.to(device)\n",
    "        self.bias = self.bias.to(device)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.inputs = input\n",
    "        return input @ self.weights.T + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Connect the grad_output with the inputs\n",
    "        self.grad_input = grad_output\n",
    "        self.grad_weights = self.grad_input.T @ self.inputs\n",
    "        self.grad_biases = self.grad_input.sum(dim=0) # summation will be across each column\n",
    "        self.grad_output = self.grad_input @ self.weights\n",
    "\n",
    "        return self.grad_output\n",
    "\n"
   ],
   "id": "d2b0bfdaf22da19b",
   "outputs": [],
   "execution_count": 303
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.375828Z",
     "start_time": "2026-01-28T23:11:37.370722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaxPool:\n",
    "    def __init__(self):\n",
    "        pass"
   ],
   "id": "e561982b70ac5c4f",
   "outputs": [],
   "execution_count": 304
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.381725Z",
     "start_time": "2026-01-28T23:11:37.376347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Convolution:\n",
    "    def __init__(self):\n",
    "        pass"
   ],
   "id": "b69b3c4d5df63814",
   "outputs": [],
   "execution_count": 305
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.388432Z",
     "start_time": "2026-01-28T23:11:37.382468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.grad_output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        zeros = torch.zeros(input.shape, device=DEVICE)\n",
    "        self.input = input\n",
    "        self.output = torch.maximum(input=self.input, other=zeros)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output\n",
    "        self.grad_output = self.grad_input * (self.output > 0)\n",
    "        return self.grad_output\n"
   ],
   "id": "8622856e9cbc20",
   "outputs": [],
   "execution_count": 306
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.395852Z",
     "start_time": "2026-01-28T23:11:37.388959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.grad_output = None\n",
    "        self.grad_input = None\n",
    "        self.zeros = torch.zeros(input.shape, device=DEVICE)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        denominator = 1 + torch.exp(-self.input)\n",
    "        self.output = 1 / denominator\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output\n",
    "        self.grad_output = self.grad_input * (self.output * (1 - self.output))\n",
    "        return self.grad_output\n"
   ],
   "id": "ff5d73b681175e16",
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.405733Z",
     "start_time": "2026-01-28T23:11:37.396665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.grad_input = None\n",
    "        self.grad_output = None\n",
    "        self.zeros = torch.zeros(input.shape, device=DEVICE)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        numerator = (torch.exp(self.input) - torch.exp(-self.input))\n",
    "        denominator = (torch.exp(self.input) + torch.exp(-self.input))\n",
    "        self.output = numerator / denominator\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output\n",
    "        self.grad_output = self.grad_input * (1 - torch.pow(self.output, 2))\n",
    "        return self.grad_output\n",
    "\n",
    "\n"
   ],
   "id": "90dccaddd4b4131b",
   "outputs": [],
   "execution_count": 308
  },
  {
   "metadata": {
    "tags": [
     "Learning Softmax Derivative"
    ],
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.414061Z",
     "start_time": "2026-01-28T23:11:37.406699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Softmax Derivative\n",
    "# # Learning about Jacobian matrices\n",
    "#\n",
    "# batch_size = 2\n",
    "# num_neurons = 3\n",
    "#\n",
    "# # 1. Creating dummy raw scores (logits)\n",
    "# # These will be the activated values from the linear layer\n",
    "#\n",
    "# logits = torch.randn(batch_size, num_neurons)\n",
    "#\n",
    "# # 2. Apply the Softmax to get the probabilities\n",
    "# # dim=1 means that the softmax is computed across the neurons per batch, so going across to calculate the softmax\n",
    "#\n",
    "# probabilities = torch.softmax(logits, dim=1)\n",
    "#\n",
    "# print(f\"logits: {logits}\")\n",
    "# print(f\"probabilities: {probabilities}\")\n",
    "# print(\"Shape: \", probabilities.shape)\n",
    "#\n",
    "# # 3. Create the Jacobian for the whole batch\n",
    "# # Step A: Diagonal part (i=j)\n",
    "# diag_part = torch.diag_embed(probabilities)\n",
    "#\n",
    "# # Step B: Outer product part (i != j)\n",
    "# # p_reshaped becomes (3,4,1)\n",
    "# p_reshaped = probabilities.unsqueeze(2)\n",
    "#\n",
    "# print(f\"p_reshaped shape: {p_reshaped.shape}\")\n",
    "# print(f\"p_reshaped: \\n{p_reshaped}\")\n",
    "#\n",
    "# # bmm performs (3,4,1) x (3,1,4) -> (3,4,4)\n",
    "#\n",
    "# p_reshaped_transposed = p_reshaped.transpose(1, 2)  # swap dimensions 1 and 2\n",
    "#\n",
    "# print(f\"p_reshaped_transposed shape: {p_reshaped_transposed.shape}\")\n",
    "# print(f\"p_reshaped_transposed:\\n {p_reshaped_transposed}\")\n",
    "#\n",
    "# outer_part = torch.bmm(p_reshaped, p_reshaped_transposed)\n",
    "# print(f\"outer_part shape: {outer_part.shape}\")\n",
    "# print(f\"outer_part: \\n {outer_part}\")\n",
    "#\n",
    "# # Step C: Combine\n",
    "# softmax_derivative = diag_part - outer_part\n",
    "# print(f\"softmax_derivative: \\n{softmax_derivative}\")"
   ],
   "id": "c9b32d2b84c14c1b",
   "outputs": [],
   "execution_count": 309
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.423809Z",
     "start_time": "2026-01-28T23:11:37.414872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.pred = None\n",
    "        self.grad_input = None\n",
    "        self.grad_output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        max_val, _ = torch.max(input, dim=1, keepdim=True)\n",
    "        shifted_input = input - max_val\n",
    "        numerator = torch.exp(shifted_input)\n",
    "        denominator = torch.sum(numerator, dim=1, keepdim=True)\n",
    "        self.pred = numerator / denominator\n",
    "\n",
    "        return self.pred\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_input = grad_output\n",
    "\n",
    "        # Creating the Jacobian for the whole batch\n",
    "        # Step A: Diagonal part (i = j)\n",
    "        diag_part = torch.diag_embed(self.pred)\n",
    "\n",
    "        # Step B: Outer product part (i != j)\n",
    "        # p_reshaped becomes (3,4,1)\n",
    "        p_reshaped = self.pred.unsqueeze(2)\n",
    "\n",
    "        # bmm performs (3,4,1) x (3,1,4) -> (3,4,4)\n",
    "        p_reshaped_transposed = p_reshaped.transpose(1, 2)  # swap dimensions 1 and 2\n",
    "        outer_part = torch.bmm(p_reshaped, p_reshaped_transposed)\n",
    "\n",
    "        softmax_derivative = diag_part - outer_part\n",
    "\n",
    "        # self.grad_output = softmax_derivative * self.grad_input\n",
    "        self.grad_output = torch.bmm(softmax_derivative, self.grad_input.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return self.grad_output\n"
   ],
   "id": "e618f1f00d0c4f6",
   "outputs": [],
   "execution_count": 310
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.432206Z",
     "start_time": "2026-01-28T23:11:37.424586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.y_true = None\n",
    "        self.pred = None\n",
    "        self.avg_loss = None\n",
    "        self.epsilon = 1e-9\n",
    "        self.batch_size = None\n",
    "        self.grad_output = None\n",
    "\n",
    "    # since our model will be trained in batches, the loss will the total loss over each sample averaged over the number of samples there are in a batch\n",
    "    def forward(self, predictions, y_true):\n",
    "        self.pred = predictions\n",
    "        self.y_true = y_true\n",
    "        self.batch_size = self.pred.shape[0] # (batch_size, neurons)\n",
    "        batch_loss = -torch.sum(self.y_true * torch.log(predictions + self.epsilon))\n",
    "        self.avg_loss = batch_loss / self.batch_size\n",
    "\n",
    "        return self.avg_loss\n",
    "\n",
    "    def backward(self):\n",
    "        cross_entropy_derivative = -(self.y_true / (self.pred + self.epsilon))\n",
    "\n",
    "        # We will divide by batch_size to average out across the batch, this way we can keep th gradient scale consistent.\n",
    "        self.grad_output = (cross_entropy_derivative / self.batch_size)\n",
    "\n",
    "        return self.grad_output\n",
    "\n",
    "\n"
   ],
   "id": "89a135a4129329d6",
   "outputs": [],
   "execution_count": 311
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.438347Z",
     "start_time": "2026-01-28T23:11:37.432959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        pass"
   ],
   "id": "c4c45943170bd690",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.445156Z",
     "start_time": "2026-01-28T23:11:37.438963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.grads = None\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grads, input):\n",
    "        self.input = input\n",
    "        self.output = self.input - (self.lr * grads)\n",
    "        return self.output\n",
    "\n"
   ],
   "id": "624c64c6db256c21",
   "outputs": [],
   "execution_count": 313
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.452077Z",
     "start_time": "2026-01-28T23:11:37.445932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "    def __init__(self):\n",
    "        pass"
   ],
   "id": "9edb51a7d89a0fcf",
   "outputs": [],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.460414Z",
     "start_time": "2026-01-28T23:11:37.452708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self, sequential, optimiser, lr=0.01 ,device=DEVICE):\n",
    "        self.model_structure = sequential\n",
    "        self.optimiser = optimiser\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.cross_entropy = CrossEntropyLoss()\n",
    "        self.y_true = None\n",
    "\n",
    "\n",
    "    def train(self, y_true, input):\n",
    "        self.y_true = y_true\n",
    "        pred = self.model_structure.forward(input)\n",
    "        # print(f\"pred: {pred}\")\n",
    "        curr_loss = self.cross_entropy.forward(pred, self.y_true)\n",
    "        # print(f\"Current loss: {curr_loss}\")\n",
    "        grad = self.cross_entropy.backward()\n",
    "\n",
    "        for layer in reversed(self.model_structure.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "            if hasattr(layer, \"weights\"):\n",
    "                layer.weights = self.optimiser.step(grads=layer.grad_weights, input=layer.weights)\n",
    "                layer.bias = self.optimiser.step(grads=layer.grad_biases, input=layer.bias)\n",
    "\n",
    "        return curr_loss"
   ],
   "id": "47d6e2660781ee76",
   "outputs": [],
   "execution_count": 315
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.553222Z",
     "start_time": "2026-01-28T23:11:37.461324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modules from torch that are needed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# We need to flatten the input data before we can pass it into the dense layers\n",
    "# For now keep it flattened but, when CNN implemented, change later\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root= \"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=  \"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create batches of data\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ],
   "id": "7143d19ddab1cf7b",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.825893Z",
     "start_time": "2026-01-28T23:11:37.553829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Batch size:\", train_loader.batch_size)\n",
    "print(\"Number of train batches (len(train_loader)):\", len(train_loader))\n",
    "\n",
    "# Check first 3 batches\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"\\nTrain batch {i}:\")\n",
    "    print(\"inputs shape:\", inputs.shape)\n",
    "    print(\"labels shape:\", labels.shape)\n",
    "    print(\"inputs dtype:\", inputs.dtype)\n",
    "    print(\"labels dtype:\", labels.dtype)\n",
    "    print(\"min/max pixel:\", inputs.min().item(), inputs.max().item())\n",
    "    print(\"unique labels sample:\\n\", labels[:32].tolist(), \"\\n\", labels[32:].tolist())\n",
    "\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "print(\"Test dataset size:\", len(test_dataset))\n",
    "print(\"Batch size:\", test_loader.batch_size)\n",
    "print(\"Number of test batches (len(test_loader)):\", len(test_loader))\n",
    "\n",
    "# Check last test batch size\n",
    "last_inputs, last_labels = None, None\n",
    "for inputs, labels in test_loader:\n",
    "    last_inputs, last_labels = inputs, labels\n",
    "\n",
    "print(\"\\nLast test batch:\")\n",
    "print(\"inputs shape:\", last_inputs.shape)\n",
    "print(\"labels shape:\", last_labels.shape)"
   ],
   "id": "8396b845e3a2ec4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60000\n",
      "Batch size: 64\n",
      "Number of train batches (len(train_loader)): 937\n",
      "\n",
      "Train batch 0:\n",
      "inputs shape: torch.Size([64, 784])\n",
      "labels shape: torch.Size([64])\n",
      "inputs dtype: torch.float32\n",
      "labels dtype: torch.int64\n",
      "min/max pixel: 0.0 1.0\n",
      "unique labels sample:\n",
      " [7, 2, 0, 2, 0, 3, 3, 6, 7, 9, 8, 8, 8, 6, 5, 0, 4, 8, 0, 9, 1, 7, 2, 9, 5, 2, 1, 1, 6, 4, 9, 4] \n",
      " [2, 9, 6, 6, 5, 4, 4, 8, 3, 2, 9, 7, 6, 0, 5, 7, 0, 5, 6, 1, 9, 6, 8, 7, 5, 8, 7, 0, 8, 2, 1, 6]\n",
      "Test dataset size: 10000\n",
      "Batch size: 64\n",
      "Number of test batches (len(test_loader)): 157\n",
      "\n",
      "Last test batch:\n",
      "inputs shape: torch.Size([16, 784])\n",
      "labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:11:37.831869Z",
     "start_time": "2026-01-28T23:11:37.826649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test\n",
    "#\n",
    "# model_features = [\n",
    "#     (\"linear\", 784, 128),\n",
    "#     (\"activation\", \"ReLU\"),\n",
    "#     (\"linear\", 128, 10),\n",
    "#     (\"activation\", \"Softmax\")\n",
    "# ]\n",
    "\n",
    "# Dummy y_true values\n",
    "# y_true = torch.tensor([\n",
    "#     [1, 0, 0, 0],  # Sample 0 is Class 0\n",
    "#     [0, 0, 0, 1],  # Sample 1 is Class 3\n",
    "#     [0, 0, 1, 0],  # Sample 2 is Class 2\n",
    "#     [0, 0, 0, 1]   # Sample 3 is Class 1\n",
    "# ], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# new_model = Sequential(model_features, device=DEVICE)\n",
    "# new_model.create_instances()\n",
    "# inputs = torch.randn(4, 4).to(DEVICE)\n",
    "# print(f\"inputs: {inputs}\")\n",
    "# # out = new_model.forward(inputs)\n",
    "# # print(f\"outputs: {out}\")\n",
    "#\n",
    "# out = new_model.forward(inputs)\n",
    "# cross_entropy_loss = CrossEntropyLoss()\n",
    "# loss = cross_entropy_loss.forward(predictions=out, y_true=y_true)\n",
    "# print(out)\n",
    "# print(loss)\n",
    "#\n",
    "# optimiser = SGD(lr=0.01)\n",
    "# initial_grad_output = cross_entropy_loss.backward()\n",
    "#\n",
    "# grad_descent = new_model.backward(y=grad_output)\n",
    "# print(grad_descent)\n",
    "\n",
    "# Dummy Test\n",
    "\n",
    "# new_arcitecture = Sequential(model_features, device=DEVICE)\n",
    "# new_arcitecture.create_instances()\n",
    "# optimiser = SGD(lr=0.01)\n",
    "# new_model = Model(sequential=new_arcitecture,optimiser=optimiser,lr=0.1, device=DEVICE)\n",
    "#\n",
    "# epochs = 1000\n",
    "#\n",
    "# for epoch in range(epochs):\n",
    "#     loss = new_model.train(y_true=y_true, input=inputs)\n",
    "#     print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "\n"
   ],
   "id": "a1725b7073d033eb",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-28T23:12:01.156823Z",
     "start_time": "2026-01-28T23:11:37.832550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_features = [\n",
    "    (\"linear\", 784, 128),\n",
    "    (\"activation\", \"ReLU\"),\n",
    "    (\"linear\", 128, 10),\n",
    "    (\"activation\", \"Softmax\")\n",
    "]\n",
    "\n",
    "network = Sequential(model_features, device=DEVICE)\n",
    "network.create_instances()\n",
    "\n",
    "optimiser = SGD(lr=0.1)\n",
    "model = Model(sequential=network, optimiser=optimiser, device=DEVICE, lr=0.1)\n",
    "\n",
    "def one_hot(labels, num_classes=10, device=DEVICE):\n",
    "    y = torch.zeros(labels.size(0),\n",
    "                    num_classes,\n",
    "                    device=device,\n",
    "                    dtype=torch.int64)\n",
    "    y.scatter_(1,labels.unsqueeze(1),1)\n",
    "    return y\n",
    "\n",
    "# Training\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        y_true = one_hot(labels, 10, DEVICE)\n",
    "\n",
    "        loss = model.train(y_true, inputs)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "        pred = model.model_structure.forward(inputs)\n",
    "        pred_labels = pred.argmax(dim=1)\n",
    "        correct += (pred_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = (correct / total) * 100\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}, Accuracy: {acc}\")\n",
    "\n",
    "# Testing\n",
    "\n",
    "model.model_structure.to(device=DEVICE)\n",
    "\n",
    "test_total_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    pred = model.model_structure.forward(inputs)\n",
    "\n",
    "    pred_labels = pred.argmax(dim=1)\n",
    "    test_correct += (pred_labels == labels).sum().item()\n",
    "    test_total += labels.size(0)\n",
    "\n",
    "    y_true = one_hot(labels, 10, DEVICE)\n",
    "    batch_loss = model.cross_entropy.forward(pred, y_true)\n",
    "    test_total_loss += batch_loss.item()\n",
    "\n",
    "test_avg_loss = test_total_loss / len(test_loader)\n",
    "test_acc = (test_correct / test_total) * 100\n",
    "print(f\"Test Loss: {test_avg_loss}, Test Accuracy: {test_acc}\")\n",
    "\n"
   ],
   "id": "ea965a37c63e7aa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Activation function: ReLU\n",
      "Created Activation function: Softmax\n",
      "Epoch 1/5, Loss: 0.543689077737937, Accuracy: 87.20984525080043\n",
      "Epoch 2/5, Loss: 0.24391738001730298, Accuracy: 94.99399679829243\n",
      "Epoch 3/5, Loss: 0.18254830757814003, Accuracy: 96.4447705442903\n",
      "Epoch 4/5, Loss: 0.14635360802115283, Accuracy: 97.32857524012807\n",
      "Epoch 5/5, Loss: 0.1216753886052454, Accuracy: 97.97058431163286\n",
      "Test Loss: 0.11341957578429608, Test Accuracy: 96.55\n"
     ]
    }
   ],
   "execution_count": 319
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
